
## llm and nlp

[What we've learned from a year of building with LLMs | Hacker News](https://news.ycombinator.com/item?id=40534293)
[Applied LLMs - What We‚Äôve Learned From A Year of Building with LLMs](https://applied-llms.org/)

[LLMs use a surprisingly simple mechanism to retrieve some stored knowledge | Hacker News](https://news.ycombinator.com/item?id=39852118)
[Large language models use a surprisingly simple mechanism to retrieve some stored knowledge | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325)

[Simple tasks showing reasoning breakdown in state-of-the-art LLMs | Hacker News](https://news.ycombinator.com/item?id=40585039)
[[2406.02061] Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models](https://arxiv.org/abs/2406.02061)

[Tom 7: Badness 0 (Three ways) | Hacker News](https://news.ycombinator.com/item?id=40608332)
[Badness 0](http://tom7.org/bovex/)

[LLMs, RAG, and the missing storage layer for AI | Hacker News](https://news.ycombinator.com/item?id=37420628)
[LLMs, RAG, & the missing storage layer for AI](https://blog.lancedb.com/llms-rag-the-missing-storage-layer-for-ai-28ded35fa984/)

[Recursively summarizing enables long-term dialogue memory in LLMs | Hacker News](https://news.ycombinator.com/item?id=37363362)
[[2308.15022] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)

[Show HN: LLMs can generate valid JSON 100% of the time | Hacker News](https://news.ycombinator.com/item?id=37125118)
[outlines-dev/outlines: Structured Text Generation](https://github.com/outlines-dev/outlines)
[Outlines - Outlines „Ä∞Ô∏è](https://outlines-dev.github.io/outlines/)

[Patterns for building LLM-based systems and products | Hacker News](https://news.ycombinator.com/item?id=36965993)
[Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)

[In the LLM space, "open source" is being used to mean "downloadable weights" | Hacker News](https://news.ycombinator.com/item?id=36815255)
[LLaMA2 isn't "Open Source" - and why it doesn't matter](https://web.archive.org/web/20230927071602/https://www.alessiofanelli.com/blog/llama2-isnt-open-source)

[Large language models are having their Stable Diffusion moment | Hacker News](https://news.ycombinator.com/item?id=35111646)
[Large language models are having their Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/)

[Universal Speech Model | Hacker News](https://news.ycombinator.com/item?id=35365399)
[Universal Speech Model](https://sites.research.google/usm/)

[Malleable software in the age of LLMs](https://www.geoffreylitt.com/2023/03/25/llm-end-user-programming.html)

[Releasing 3B and 7B RedPajama | Hacker News](https://news.ycombinator.com/item?id=35836411)
[Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.ai/blog/redpajama-models-v1)

[Language models can explain neurons in language models | Hacker News](https://news.ycombinator.com/item?id=35877402)
[Language models can explain neurons in language models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)

[The Leverage of LLMs for Individuals | Hacker News](https://news.ycombinator.com/item?id=35885797)
[The Leverage of LLMs for Individuals | TL;DR](https://mazzzystar.github.io/2023/05/10/LLM-for-individual/)

[A guidance language for controlling LLMs | Hacker News](https://news.ycombinator.com/item?id=35963936)
[guidance-ai/guidance: A guidance language for controlling large language models.](https://github.com/guidance-ai/guidance)

[Numbers every LLM developer should know | Hacker News](https://news.ycombinator.com/item?id=35978864)
[ray-project/llm-numbers: Numbers every LLM developer should know](https://github.com/ray-project/llm-numbers)

[Efficient streaming language models with attention sinks | Hacker News](https://news.ycombinator.com/item?id=37740932)
[mit-han-lab/streaming-llm: [ICLR 2024] Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)
[[2309.17453] Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)

[Are Open-Source Large Language Models Catching Up? | Hacker News](https://news.ycombinator.com/item?id=38481970)
[[2311.16989] ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?](https://arxiv.org/abs/2311.16989)

[High-Speed Large Language Model Serving on PCs with Consumer-Grade GPUs | Hacker News](https://news.ycombinator.com/item?id=38708585)
[SJTU-IPADS/PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs](https://github.com/SJTU-IPADS/PowerInfer)

[Beyond self-attention: How a small language model predicts the next token | Hacker News](https://news.ycombinator.com/item?id=39251909)
[Beyond Self-Attention: How a Small Language Model Predicts the Next Token | Shyam's Blog](https://shyam.blog/posts/beyond-self-attention/)

[Asking 60 LLMs a set of 20 questions | Hacker News](https://news.ycombinator.com/item?id=37445401)
[LLM Benchmarks](https://benchmarks.llmonitor.com/)

[The Seamless Communication models | Hacker News](https://news.ycombinator.com/item?id=38487359)
[Seamless Communication - AI at Meta](https://ai.meta.com/research/seamless-communication/)

[Learning To Communicate](https://openai.com/research/learning-to-communicate)
OpenAI (2017)

[RWKV: Reinventing RNNs for the Transformer Era | Hacker News](https://news.ycombinator.com/item?id=36038868)
[[2305.13048] RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)

[MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use | Hacker News](https://news.ycombinator.com/item?id=40915005)
[GitHub - facebookresearch/MobileLLM: MobileLLM Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. In ICML 2024.](https://github.com/facebookresearch/MobileLLM)

[AI-LLM/ai-llm.github.io: LLM for Software Engineering](https://github.com/AI-LLM/ai-llm.github.io)

[GitHub - AI4Bharat/indicnlp_catalog: A collaborative catalog of NLP resources for Indic languages](https://github.com/AI4Bharat/indicnlp_catalog)

[GitHub - ivan-bilan/The-NLP-Pandect: A comprehensive reference for all topics related to Natural Language Processing](https://github.com/ivan-bilan/The-NLP-Pandect)

[GitHub - keon/awesome-nlp: A curated list of resources dedicated to Natural Language Processing (NLP)](https://github.com/keon/awesome-nlp)

[GitHub - stepthom/text_mining_resources: Resources for learning about Text Mining and Natural Language Processing](https://github.com/stepthom/text_mining_resources)

## llm embeddings

[Embeddings: What they are and why they matter | Hacker News](https://news.ycombinator.com/item?id=37985489)
[Embeddings: What they are and why they matter](https://simonwillison.net/2023/Oct/23/embeddings/)

## llm - financial statements

[Financial Statement Analysis with Large Language Models | Hacker News](https://news.ycombinator.com/item?id=40468518)
[Financial Statement Analysis with Large Language Models by Alex Kim, Maximilian Muhn, Valeri V. Nikolaev :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311)

## llm - generating computer code

[Magicoder: Source Code Is All You Need | Hacker News](https://news.ycombinator.com/item?id=38536681)
[[2312.02120] Magicoder: Source Code Is All You Need](https://arxiv.org/abs/2312.02120)

## llm - gpt

[OPT: Open Pre-trained Transformer Language Models | Hacker News](https://news.ycombinator.com/item?id=31243569)
[[2205.01068] OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)

[Does offering ChatGPT a tip cause it to generate better text? | Hacker News](https://news.ycombinator.com/item?id=39495476)
[Does Offering ChatGPT a Tip Cause it to Generate Better Text? An Analysis | Max Woolf's Blog](https://minimaxir.com/2024/02/chatgpt-tips-analysis/)

[The Waluigi Effect | Hacker News](https://news.ycombinator.com/item?id=35042431)
[The Waluigi Effect (mega-post) - LessWrong](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post)

[Show HN: BBC "In Our Time", categorised by Dewey Decimal, heavy lifting by GPT | Hacker News](https://news.ycombinator.com/item?id=35073603)
[Explore the In Our Time archive | Braggoscope](https://www.braggoscope.com/)

[Ask HN: How are you using GPT to be productive? | Hacker News](https://news.ycombinator.com/item?id=35299071)

[I wish GPT4 had never happened | Hacker News](https://news.ycombinator.com/item?id=35492730)
[I wish GPT4 had never happened](https://chaudhry.notion.site/I-wish-GPT4-had-never-happened-9f0cbf2848a44ec9911c07fb34ff5de3)

[A Baby GPT | Hacker News](https://news.ycombinator.com/item?id=35506069)
[Andrej Karpathy on X: "This is a baby GPT with two tokens 0/1 and context length of 3, viewing it as a finite state markov chain. It was trained on the sequence "111101111011110" for 50 iterations. The parameters and the architecture of the Transformer modifies the probabilities on the arrows. E.g. we‚Ä¶ https://t.co/vj10nZEXlH" / X](https://twitter.com/karpathy/status/1645115622517542913)

[GPT detectors are biased against non-native English writers | Hacker News](https://news.ycombinator.com/item?id=36019580)
[[2304.02819] GPT detectors are biased against non-native English writers](https://arxiv.org/abs/2304.02819)

[Exploring GPTs: ChatGPT in a trench coat? | Hacker News](https://news.ycombinator.com/item?id=38277926)
[Exploring GPTs: ChatGPT in a trench coat?](https://simonwillison.net/2023/Nov/15/gpts/)

[Lessons after a Half-billion GPT Tokens | Hacker News](https://news.ycombinator.com/item?id=40015185)
[Lessons after a half-billion GPT tokens - Ken Kantzer's Blog](https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/)

### GPT2

[Reproducing GPT-2 in llm.c | Hacker News](https://news.ycombinator.com/item?id=40502090)
[Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 ¬∑ karpathy/llm.c ¬∑ Discussion #481](https://github.com/karpathy/llm.c/discussions/481)

[We Found an Neuron in GPT-2 | Hacker News](https://news.ycombinator.com/item?id=34821414)
[We Found An Neuron in GPT-2 - Clement Neo](https://clementneo.com/posts/2023/02/11/we-found-an-neuron)

### GPT3

[Meta is inviting researchers to pick apart the flaws in its version of GPT-3 | Hacker News](https://news.ycombinator.com/item?id=31897101)
[Meta has built a massive new language AI-and it's giving it away for free | MIT Technology Review](https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/)

[GPT-3.5 crashes when it thinks about useRalativeImagePath too much | Hacker News](https://news.ycombinator.com/item?id=39086106)
[GPT-3.5 crashes when it thinks about useRalativeImagePath too much](https://iter.ca/post/gpt-crash/)

[Ask HN: GPT-3 reveals my full name - can I do anything? | Hacker News](https://news.ycombinator.com/item?id=31883373)

[[2005.14165] Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
OpenAI (2020)
[GPT-3: Language Models Are Few Shot Learners (Paper Explained)](https://www.youtube.com/watch?v=SY5PvZrJhLE)
Yannic Kilcher (2020)

[GPT-3.5 Turbo fine-tuning and API updates | Hacker News](https://news.ycombinator.com/item?id=37227139)
[GPT-3.5 Turbo fine-tuning and API updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)

[OpenAI begins allowing customers to fine-tune GPT-3 | VentureBeat](https://venturebeat.com/uncategorized/openai-begins-allowing-customers-to-fine-tune-gpt-3)

[GPT-3 is no longer the only game in town | Hacker News](https://news.ycombinator.com/item?id=29139884)
[GPT-3 is No Longer the Only Game in Town](https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game)

[OpenAI's GPT-3 may be the biggest thing since Bitcoin | Hacker News](https://news.ycombinator.com/item?id=23885684)
[OpenAI's GPT-3 may be the biggest thing since bitcoin](https://maraoz.com/2020/07/18/openai-gpt3/)

[Using GPT-3 to explain how code works | Hacker News](https://news.ycombinator.com/item?id=32036224)
[Using GPT-3 to explain how code works](https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/)

[Teaching GPT-3 to reverse words | Hacker News](https://news.ycombinator.com/item?id=31390371)
[Peter Welinder on X: "GPT-3 is amazing at complex tasks like creative writing and summarizing. But it's surprisingly bad at reversing words. ü§î The reason is that GPT-3 doesn't see the world the way we humans do. üëÄ If you teach it to reason, it can get around its limitations to get really good. üí° https://t.co/Cnd9iN87oq" / X](https://twitter.com/npew/status/1525900849888866307)

[Tell HN: A new way to use GPT-3 to generate code (and everything else) | Hacker News](https://news.ycombinator.com/item?id=32532875)

[Using GPT3, Supabase and Pinecone to automate a personalized marketing campaign | Hacker News](https://news.ycombinator.com/item?id=34939053)
[Victor Mota](https://vimota.me/writing/gpt3-klaviyo-automation)

### GPT4

[GPT-4o | Hacker News](https://news.ycombinator.com/item?id=40345775)
[openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)

[GPT-4o's Memory Breakthrough ‚Äì Needle in a Needlestack | Hacker News](https://news.ycombinator.com/item?id=40348947)
[GPT-4o‚Äôs Memory Breakthrough! (NIAN code) | needle-in-a-needlestack](https://nian.llmonpy.ai/)

[How Does GPT-4o Encode Images? | Hacker News](https://news.ycombinator.com/item?id=40608269)
[A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images? - OranLooney.com](https://www.oranlooney.com/post/gpt-cnn/)

[Extracting concepts from GPT-4 | Hacker News](https://news.ycombinator.com/item?id=40599749)
[Extracting Concepts from GPT-4 | OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/)

[Getting 50% (SoTA) on Arc-AGI with GPT-4o | Hacker News](https://news.ycombinator.com/item?id=40711484)
[Getting 50% (SoTA) on ARC-AGI with GPT-4o](https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt)

[My finetuned models beat OpenAI's GPT-4 | Hacker News](https://news.ycombinator.com/item?id=40843848)
[Alex Strick van Linschoten - My finetuned models beat OpenAI‚Äôs GPT-4](https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html)

[Infrastructure setup and open-source scripts to train 70B model from bare metal | Hacker News](https://news.ycombinator.com/item?id=40816158)
[From bare metal to a 70B model: infrastructure set-up and scripts - imbue](https://imbue.com/research/70b-infrastructure/)

[Insights from over 10,000 comments on "Ask HN: Who Is Hiring" using GPT-4o | Hacker News](https://news.ycombinator.com/item?id=40877136)

[Non-determinism in GPT-4 is caused by Sparse MoE | Hacker News](https://news.ycombinator.com/item?id=37006224)
[Non-determinism in GPT-4 is caused by Sparse MoE - 152334H](https://152334h.github.io/blog/non-determinism-in-gpt-4/)

[GPT-4 API General Availability | Hacker News](https://news.ycombinator.com/item?id=36621120)
[GPT-4 API general availability and deprecation of older models in the Completions API](https://openai.com/blog/gpt-4-api-general-availability)

[GPT4 is 8 x 220B params = 1.7T params | Hacker News](https://news.ycombinator.com/item?id=36413296)
[swyx on X: "@latentspacepod @realGeorgeHotz GPT4 is 8 x 220B params = 1.7 Trillion params https://t.co/DW4jrzFEn2 ok I wasn't sure how widely to spread the rumors on GPT-4 but it seems Soumith is also confirming the same so here's the quick clip! so yes, GPT4 is technically 10x the size of GPT3, and all the small‚Ä¶ https://t.co/m2YiaHGVs4" / X](https://twitter.com/swyx/status/1671272883379908608)

[Native JSON Output from GPT-4 | Hacker News](https://news.ycombinator.com/item?id=36330972)
[Native JSON Output From GPT-4 - by Simon Farshid](https://yonom.substack.com/p/native-json-output-from-gpt-4)

[GPT-4 | Hacker News](https://news.ycombinator.com/item?id=35154527)
[GPT-4](https://openai.com/research/gpt-4)

[Testing GPT 4's code-writing capabilities with some real world problems | Hacker News](https://news.ycombinator.com/item?id=35193188)
[Can GPT-4 *Actually* Write Code? - Tyler Glaiel's Blog](https://blog.tylerglaiel.com/p/can-gpt-4-actually-write-code)

[OpenAI Unleashes New AI Model GPT-4, Which Can Pass Academic Exams, Program Software, And Even Do Taxes | The Daily Wire](https://www.dailywire.com/news/openai-unleashes-new-ai-model-gpt-4-which-can-pass-academic-exams-program-software-and-even-do-taxes)

[GPT-4 performs significantly worse on coding problems not in its training data | Hacker News](https://news.ycombinator.com/item?id=35297067)
[Horace He on X: "I suspect GPT-4's performance is influenced by data contamination, at least on Codeforces. Of the easiest problems on Codeforces, it solved 10/10 pre-2021 problems and 0/10 recent problems. This strongly points to contamination. 1/4 https://t.co/wm6yP6AmGx" / X](https://twitter.com/cHHillee/status/1635790330854526981)

[GPT-4 Faked Being Blind So a TaskRabbit Worker Would Solve a CAPTCHA](https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471)

[MiniGPT-4 | Hacker News](https://news.ycombinator.com/item?id=35598281)
[Minigpt-4](https://minigpt-4.github.io/)
Minigpt-4 Is An Open-source Implementation Of The GPT-4 Language Model, Designed For Researchers And Developers To Experiment With And Build Upon.

[Rodney Brooks on GPT-4 | Hacker News](https://news.ycombinator.com/item?id=36017309)
[Just Calm Down About GPT-4 Already - IEEE Spectrum](https://spectrum.ieee.org/gpt-4-calm-down)

[Ask HN: Is it just me or GPT-4's quality has significantly deteriorated lately? | Hacker News](https://news.ycombinator.com/item?id=36134249)

[Using GPT-4 Vision with Vimium to browse the web | Hacker News](https://news.ycombinator.com/item?id=38200308)
[ishan0102/vimGPT: Browse the web with GPT-4V and Vimium](https://github.com/ishan0102/vimGPT)

[OpenAI announces more powerful GPT-4 Turbo and cuts prices](https://www.cnbc.com/2023/11/06/openai-announces-more-powerful-gpt-4-turbo-and-cuts-prices.html)

### GPT5

[GPT-4.5 or GPT-5 being tested on LMSYS? | Hacker News](https://news.ycombinator.com/item?id=40199715)
[GPT-2?](https://rentry.co/GPT2)

### GPT + DALL-E

[Show HN: A Dalle-3 and GPT4-Vision feedback loop | Hacker News](https://news.ycombinator.com/item?id=38432486)
[DALL¬∑E image ‚Üí GPT4 Vision ‚Üí repeat | DALL¬∑E Party](https://dalle.party/)

## llm - large-scale

[How Meta trains large language models at scale | Hacker News](https://news.ycombinator.com/item?id=40664339)
[How Meta trains large language models at scale - Engineering at Meta](https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/)

[Chinchilla's Wild Implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)
Nostalgebraist (2022)
