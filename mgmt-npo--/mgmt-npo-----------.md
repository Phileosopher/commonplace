
#### Elsewhere in misalignment - Matt Levine

We [have](https://www.bloomberg.com/opinion/articles/2023-11-20/who-controls-openai) [talked](https://www.bloomberg.com/opinion/articles/2023-11-21/openai-is-a-strange-nonprofit) a [lot](https://www.bloomberg.com/opinion/articles/2023-11-27/openai-is-still-an-86-billion-nonprofit) [about](https://www.bloomberg.com/opinion/articles/2023-11-28/the-sec-might-lose-its-courts) the recent drama at OpenAI, whose nonprofit board of directors fired, and were then in turn fired by, its chief executive officer Sam Altman. Here is [Ezra Klein on the board's motivations](https://www.threads.net/@ezraklein/post/C0MpLJNuN7U):

> One thing in the OpenAI story I am now fully convinced of, as it's consistent in my interviews on both sides.
>
> This was not about safety. It was not about commercialization. It was not about speed of development or releases. It was not about Q*. It was really a pure fight over control.
>
> The board felt it couldn't control/trust Altman. It felt Altman could and would outmaneuver them in a pinch. But he wasn't outmaneuvering them on X issue. They just felt they couldn't govern him.

Well, sure, but that _is_ a fight about AI safety. It's just a _metaphorical_ fight about AI safety. I am sorry, [I have made this joke before](https://www.bloomberg.com/opinion/articles/2023-11-27/openai-is-still-an-86-billion-nonprofit), but events keep sharpening it. The OpenAI board looked at Sam Altman and thought "this guy is smarter than us, he can outmaneuver us in a pinch, and it makes us nervous. He's done nothing wrong so far, but we can't be sure what he'll do next as his capabilities expand. We do not fully trust him, we cannot fully control him, and we do not have a model of how his mind works that we fully understand. Therefore we have to shut him down before he grows too powerful."

I'm sorry! That is exactly the AI misalignment worry! If you spend your time managing AIs that are growing exponentially smarter, you might worry about losing control of them, and if you spend your time managing Sam Altman you might worry about losing control of him, and if you spend your time managing both of them you might get confused about which is which. Maybe Sam Altman will turn the old board members into [paper clips](https://en.wikipedia.org/wiki/Instrumental_convergence).

Elsewhere in OpenAI, [the Information reports](https://www.theinformation.com/articles/openai-isnt-expected-to-offer-microsoft-other-investors-a-board-seat) that the board will remain pretty nonprofit-y:

> OpenAI's revamped board of directors doesn't plan to include representatives from outside investors, according to a person familiar with the situation. It's a sign that the board will prioritize safety practices ahead of investor returns.
>
> The new board hasn't been officially seated and things could change. But the person said Microsoft and other shareholders, such as Khosla Ventures, Thrive Capital and Sequoia Capital, aren't expected to be offered a seat on OpenAI's new nine-person board. 

Still. [I think that](https://www.bloomberg.com/opinion/articles/2023-11-27/openai-is-still-an-86-billion-nonprofit) the OpenAI board two weeks ago (1) did not include any investor representatives and (2) was _fundamentally unpredictable to investors_ - it might have gone and fired Altman! - whereas the future OpenAI board (1) will not include any investor representatives but (2) will nonetheless be a bit more constrained by the investors' interests. "If we are _too_ nonprofit-y, the company will vanish in a puff of smoke, and that will be bad," the new board will think, whereas the old board actually went around saying things like "allowing the company to be destroyed would be consistent with the mission" and almost meant it. The investors don't exactly need a board seat if they have a practical veto over the board's biggest decisions, and the events of the last two weeks suggest that they do.
