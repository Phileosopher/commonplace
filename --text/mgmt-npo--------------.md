
#### OpenAI: Startup governance - Matt Levine

Obviously another way to look at the OpenAI situation is that OpenAI is an $86 billion tech startup that did some real odd stuff to incinerate most of its value. 

In some ways this is not _that_ unusual a story of nonprofit governance, in which the board's abstract commitment to the mission conflicts with the practical on-the-ground experiences of the staff. But it sure is an unusual story of startup governance, in which Microsoft agreed to invest $13 billion in OpenAI, and other venture capitalists also put in money, and employees got stock grants, without the contractual or fiduciary rights that investors would normally get. And then one day the board of OpenAI was like "hey we decided to blow up the company" and the investors were like "wait a minute can you really do that" and the board was like "oh yeah sure we can." So [Bloomberg reports](https://www.bloomberg.com/news/articles/2023-11-20/openai-staff-threaten-to-go-to-microsoft-if-board-doesn-t-quit) that "some investors were considering writing down the value of their OpenAI holdings to zero." Eighty-six billion dollars of value evaporated in a weekend.

One popular thing to say about this is that the investors should have been more careful about governance, and that future investors in future startups will pay more attention to things like control rights and fiduciary duties and board composition. And, maybe. But I have to say I sympathize with the investors here. There are many cases in which sophisticated investors invest large sums of money into companies where they have no real control rights, and they _rationally_ calculate that it will be fine. Generally the calculation will involve some combination of factors like:

1. I have met the founder and shook her hand and looked into her eyes and I _trust_ her, so I do not need to care about the corporate formalities. (Smart investors jumped into [Elon Musk's Twitter Inc. adventure](https://www.bloomberg.com/opinion/articles/2022-10-03/everyone-wanted-to-buy-twitter-with-elon) not because they did extensive due diligence or got a lot of control rights, but because he's Elon Musk.)
2. Regardless of the formalities, the _incentives_ are on my side: This company will need more money, the founder will need to sell her shares, and so even if she has the formal right to hose me she won't, because that will be bad for her. (Both Adam Neumann and Travis Kalanick were forced out of startups that they had founded and where they had more or less total formal control, because their investors told them "hey look if you keep your total control this is going to be a zero, whereas if you leave now you can salvage some value for yourself," and they made a rational choice.)
3. The formalities are bad, sure, but that's the price of getting into this investment, and the _upside_ of this investment is so huge that I am willing to take the risk of getting hosed by bad governance. (Early investors in Facebook Inc., now Meta Platforms Inc., had very little in the way of governance rights, Mark Zuckerberg had total control, and guess what he still does and he has made those investors very rich.)

It is not hard to see how OpenAI's investors could have had similar thoughts:

1. They really liked Sam Altman! He is a popular and well-connected figure among venture capital types. Nor, really, were they wrong to trust him. It's just that usually startup founders have much more control of their boards than Altman did. That _did_ turn out to be a failure of organizational due diligence by OpenAI's investors, but an understandable one.
2. The incentives were just incredibly on their side. OpenAI requires piles and piles of outside money to do its work, so it cannot rationally afford to alienate investors. Microsoft, OpenAI's biggest investor, also provides its computing power and has a license to its technology and, after this weekend's implosion, seems to be on track to hire most of its staff. "You can make the case that Microsoft just acquired OpenAI for $0 and zero risk of an antitrust lawsuit," [Ben Thompson wrote yesterday](https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/). No rational startup would let that happen! Meanwhile Thrive Capital was [leading the tender offer](https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/) to buy employee shares, and might have thought "these employees need liquidity and will not bite the hand that is feeding them." These were all, I think, very reasonable things to think; they were just flummoxed by a board that _did not act in the economic best interests of the company_. Again: because it wasn't supposed to! Still a jarring surprise.
3. The upside was really big. I mean the company was worth more than $80 billion last week, not because it was profitable (it was a money pit) but because, you know, it had an [8% chance of being a trillion-dollar company](https://www.bloomberg.com/opinion/articles/2023-11-09/adam-neumann-is-so-good-at-this). You'd take some governance risk for that upside.

I feel like the lesson here is not so much "don't invest in startups without vetting the board and ideally getting a board seat for yourself," and more "don't invest in nonprofits at an $86 billion valuation." Which I think has never come up before? Like as far as I can tell no one in human history has ever purchased shares in a nonprofit at an $86 billion valuation? Because purchasing shares in a nonprofit, at any valuation, is not a coherent thing to do? But then OpenAI made it happen, for the first time, and probably also the last.

There are other oddities here that I do not really understand but want to mention as puzzles. Like, the status as of this morning really does seem to be that Sam Altman has been fired by OpenAI, that he plans to go work at Microsoft (OpenAI's biggest investor and partner), and that he is going to take the large majority of OpenAI's employees with him. What â€¦ what do everyone's contracts look like here? Like:

- When Microsoft signed its deal with OpenAI, did it agree to any sort of non-compete or non-solicit, like, "you won't hire away our employees"?
- Does Altman have any sort of non-solicit, like, "if you leave you won't hire everyone else"?
- Do the employees have any sort of non-disclosure agreements, like, "if you leave you won't take our proprietary technology with you"? Perhaps this one doesn't matter, since Microsoft has a license to OpenAI's intellectual property: The OpenAI employees can leave, take nothing with them, and then go work at Microsoft and have access to everything they left behind. But Bloomberg's [Austin Carr reports](https://www.bloomberg.com/news/articles/2023-11-21/microsoft-hiring-sam-altman-presents-new-challenges-for-company) that "any employees who do join Microsoft can't simply replicate the work they were doing on OpenAI properties like GPT-5 without inviting a nightmare of claims over trade-secret theft."

Coming from the world of finance, all of this feels odd to me; ordinarily there would be contracts preventing a company's biggest customer from hiring its CEO and him then bringing over his whole team to build the same cutting-edge technology they were building at the company. Here, I guess, there aren't? Everyone just trusted each other? Seems like a mistake.
