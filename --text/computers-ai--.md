
https://wng.org/podcasts/brad-littlejohn-the-soul-stakes-of-ai-1754412500

https://techxplore.com/news/2025-08-ai-breakthrough-power-images.html

https://buttondown.com/untools/archive/why-ai-is-making-us-worse-thinkers-and-how-to/

https://www.thebrink.me/chatgpt-induced-psychosis-how-ai-companions-are-triggering-delusion-loneliness-and-a-mental-health-crisis-no-one-saw-coming/

[Should we use AI and LLMs for Christian Apologetics? - lukeplant.me.uk](https://lukeplant.me.uk/blog/posts/should-we-use-llms-for-christian-apologetics/)

AI is essentially reproducing "average" behavior
- it isn't going to create anything that hasn't been done many times before
- it's a dumb computer still, so it's just following rules and does NOT have an intuition for anything
- any assertions to the contrary are [trends] spurting up by self-interested individuals who want to sell something

AI hasn't been new
- 1966 saw machine translation
- it was a total dud, with lots of hype that went nowhere
- these trends create an "AI winter" where the trend and hype dies, so funding gets cut
- the entire thing is driven by a misrepresentation of what constitutes a human soul (i.e., naturalism), and how it can be reproduced
- we can reproduce intelligent behavior, but it's always dumb as it does it
- Lisp was originally the program of the artificial intelligence language
    - it failed, and we're left with emacs
    - in the late 1980s, the AI revolution in Japan collapsed, and all venture capital that even MENTIONED ai was rejected outright as "wide-eyed dreaming"

Black box vs explainable AI
- black box AI is popular, and the designers CAN'T describe what happens because the [algorithm] has too many complexities to reliably tell them (so they have to approximately guess where-ish a bug might be).
- explainable AI (XAI) is easy to [diagnose], by contrast, but also will quickly reveal any [agenda](bad systems) an individual or organization may have.

If you get past the hype, AI does 1 thing now that changes everything:
- It allows all the mundane tasks the computer used to do be much more powerful toward small permutations.
- Very specific, very analytical tasks now have the power to be parsed or compiled more easily.

AI is a bit like [engineering] in general
- we don't interact with the result directly, but are able to have a secondary interaction with it using a tool
    - that tool isn't "us", but we still command it and there is a defined order that clarifies how it's built
    - therefore, we never suspect an engineered object will go out of control without a clear and obvious reason
- in AI, we have a secondary interaction with a logical extension, which is still bound by some form of logic
    - that tool isn't "our thinking", but we still command it and there is a defined order that clarifies its logic
    - therefore, we should never suspect an AI will go out of control without a clear and obvious originating external cause

[A guide to why advanced AI could destroy the world - Vox](https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction)
- reminiscent of CGP Grey's "Humans Need Not Apply"
- the problem is that it's logic-based emotions (very granular) instead of emotions-based logic (i.e., things even toddlers can outperform adequately-enough)
- the modularity of human behavior is its engineered advantage over a specialized computer implementation

[Instrumental convergence - Wikipedia](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer)
- i.e., even AI are subject to aspects of perverse incentive

---

The "how" of machine learning, as an abstraction, is pretty easy to understand:
1. Give a computer many, many instances of a labeled thing (e.g., a cat photo).
2. Eventually, the computer makes rules that it cross-references that define those instances (e.g., fuzzy-looking, ear shape).
3. After enough crap thrown at it, it'll be able to get reliable at guessing (e.g., 80.2% accurate).
4. It's relatively trivial to make it rebuild a variation at this point.

This basically means "averaged-out" behavior, so it'll do average everything.

The implications, then, are reasonable to understand:
1. It can find all sorts of materials that were never discovered, just because it's filling in the blanks.
2. It can make average art, which cherry-picked good stuff in it (e.g., @CSB)
3. It can make documents or coding, but probably needs a human to review it.
4. It'll make a hell of a security camera.
5. It's still a dumb computer.

---

AI is largely a branding, a bit like "cloud" or ".com" was
- from a [marketing] standpoint, anything logic-based can be spun as "AI"
- the only "true" AI, though, is machine learning, and it's still not really technically "intelligence" in the way we interpret the definition of [intelligence as determined by sentient people](humanity)

## ai approach

Most AI researchers and neuroscientists guess that the quickest route to superintelligence is to bypass brain emulation and engineer it in some other way.
After all, why should our simplest path to a new technology be the one that evolution came up with.
The aviation industry didn't start with mechanical birds.
- THIS IS FUNNY BECAUSE THE WRIGHT BROTHERS SUCCEEDED BY OBSERVING BIRDS ALL DAY

A fast AI takeoff makes world takeover easier, while a slow one makes an outcome with many competing players more likely.

Consciousness is by far the most remarkable trait.
It's how our Universe gets meaning. Galaxies are beautiful only because we see and subjectively experience them.
If in the distant future our cosmos has been settled by high-tech zombie AIs, then it doesn't matter how fancy their intergalactic architecture is: it won't be beautiful or meaningful, because there's nobody and nothing to experience it - it's all just a huge and meaningless waste of space.

There are computer tournaments in so-called losing chess.

A human-extinction scenario that some people may feel better about: viewing the AI as our descendants.
Parents with a child smarter than them, who learns from them and accomplishes what they could only dream of, are likely happy and proud even if they know they can't live to see it all.
In this spirit, AIs replace humans but give us a graceful exit that makes us view them as our worthy descendants.
Humans are gradually phased out via a global one-child policy, but are treated so exquisitely well until the end that they feel they're in the most fortunate generation ever.
As long as the AIs eliminate poverty and give all humans the opportunity to live full and inspiring lives.

The only viable path to broad relinquishment of technology is to enforce it through a global totalitarian state.
If some but not all relinquish a transformative technology, then the nations or groups that defect will gradually gain enough wealth and power to take over.

Unambitious civilizations simply become cosmically irrelevant.
Almost all life that exists will be ambitious life.

There are two mathematically equivalent ways of describing each physical law: either as the past causing the future, or as nature optimizing something.
The second way is more elegant and profound.

Goal-oriented behavior was hardwired in the very laws of physics.
To rescue a swimmer as fast as possible, a lifeguard won't go in a straight line, but a bit further along the beach where she can go faster than in the water.
(Nature does this too.)

The second law of thermodynamics states that entropy tends to increase until it reaches its maximum possible value.
When you pour cold milk into hot coffee, for example, your beverage appears to march irreversibly toward its own personal heat death goal, and before long, it's all just a uniform lukewarm mixture.
If a living organism dies, its entropy also starts to rise, and before long, the arrangement of its particles tends to get much less organized.

Gravity behaves differently from all other forces and strives to make our Universe not more uniform and boring but more clumpy and interesting.

Dissipation-driven adaptation:
Random groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible.
Molecules exposed to sunlight would over time tend to arrange themselves to get better and better at absorbing sunlight.
In other words, nature appears to have a built-in goal of producing self-organizing systems that are increasingly complex and lifelike.

The second law of thermodynamics has a life loophole: although the total entropy must increase, it's allowed to decrease in some places as long as it increases even more elsewhere.
So life maintains or increases its complexity by making its environment messier.

There are many known examples of such emergent self-replication. For example:
Vortices in turbulent fluids can make copies of themselves, and clusters of microspheres can coax nearby spheres into forming identical clusters.
At some point, a particular arrangement of particles got so good at copying itself that it could do so almost indefinitely by extracting energy and raw materials from its environment.
We call such a particle arrangement life.

A living organism is an agent of bounded rationality that doesn't pursue a single goal, but instead follows rules of thumb for what to pursue and avoid.
Our human minds perceive these evolved rules of thumb as feelings, which guide our decision making toward the ultimate goal of replication.
Feelings of hunger and thirst protect us from starvation and dehydration, feelings of pain protect us from damaging our bodies, feelings of lust make us procreate, feelings of love and compassion make us help other carriers of our genes and those who help them and so on.
Guided by these feelings, our brains can quickly and efficiently decide what to do without having to subject every choice to a tedious analysis of its ultimate implications for how many descendants we'll produce.
The ultimate authority is now our feelings, not our genes.
Human behavior strictly speaking doesn't have a single well-defined goal at all.

If you'd been observing Earth's atoms since our planet formed, you'd have noticed three stages of goal-oriented behavior:
1. All matter seemed focused on dissipation (entropy increase).
2. Some of the matter came alive and instead focused on replication and subgoals of that.
3. A rapidly growing fraction of matter was rearranged by living organisms to help accomplish their goals.

The real risk with Artificial General Intelligence isn't malice but competence.
A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble.

Three tough subproblems:
1. Making AI learn our goals
2. Making AI adopt our goals
3. Making AI retain our goals

Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold.
In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: "Please undo the first two wishes, because that's not what I really wanted."
To figure out what people really want, you can't merely go by what they say.
You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious.
Once we have such a world model, we can often figure out what people want even if they don't tell us, simply by observing their goal-oriented behavior.

Children of hypocrites learn more from what they see their parents do than from what they hear them say.

We are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene.
For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he's spared the hassle of having to explain everything with words or computer programming.
One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer.
Another challenge is making machines that can figure out which particular system best matches the behavior they observe.
Inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals.
By observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences.

The time window during which you can load your goals into an AI may be quite short: the brief period between when it's too dumb to get you and too smart to let you.

A superintelligent AI will resist being shut down if you give it any goal that it needs to remain operational to accomplish - and this covers almost all goals!
If you give a superintelligence the sole goal of minimizing harm to humanity, for example, it will defend itself against shutdown attempts because it knows we'll harm one another much more in its absence through future wars and other follies.

The propensity to change goals in response to new experiences and insights increases rather than decreases with intelligence.

The ethical views of many thinkers can be distilled into four principles:
• Utilitarianism: Positive conscious experiences should be maximized and suffering should be minimized.
• Diversity: A diverse set of positive experiences is better than many repetitions of the same experience, even if the latter has been identified as the most positive experience possible.
• Autonomy: Conscious entities/societies should have the freedom to pursue their own goals unless this conflicts with an overriding principle.
• Legacy: Compatibility with scenarios that most humans today would view as happy, incompatibility with scenarios that essentially all humans today would view as terrible.

Would we really want people from 1,500 years ago to have a lot of influence over how today's world is run?
If not, why should we try to impose our ethics on future beings that may be dramatically smarter than us?

If some sophisticated future computer programs turn out to be conscious, should it be illegal to terminate them?
If there are rules against terminating digital life forms, then need there also be restrictions on creating them to avoid a digital population explosion?

A fast-forward replay of our 13.8-billion-year cosmic history:
1. Matter seemingly intent on maximizing its dissipation
2. Primitive life seemingly trying to maximize its replication
3. Humans pursuing not replication but goals related to pleasure, curiosity, compassion and other feelings that they'd evolved to help them replicate
4. Machines built to help humans pursue their human goals

The only currently programmable goals that are guaranteed to remain truly well-defined as an AI gets progressively more intelligent are goals expressed in terms of physical quantities alone, such as particle arrangements, energy and entropy.
However, humans are a historical accident, and aren't the optimal solution to any well-defined physics problem.

How should we strive to shape the future of our Universe?
If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us.
This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation!
Philosophy with a deadline.

Many arguments generate more heat than light.
(because the antagonists are talking past each other.)

Galileo described nature as "a book written in the language of mathematics."

Scientists started taking Newton's theory of gravity seriously because they got more out of it than they put into it.
Simple equations could accurately predict the outcome of every gravity experiment ever conducted.

Emergent phenomenon has properties above and beyond those of its particles.
Wetness: A drop of water is wet, but an ice crystal and a cloud of steam aren't, even though they're made of identical water molecules.
Why? Because the property of wetness depends only on the arrangement of the molecules, the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid.
What particle arrangements are conscious?
Consciousness is an emergent phenomenon.
Consciousness is the way that information feels when it's processed in certain ways.
It must be substrate-independent; it's only the structure of the information processing that matters, not the structure of the matter doing the information processing.

Which particle arrangements are conscious and which aren't?
If we can answer that, then we can figure out which AI systems are conscious.
It can also help emergency-room doctors determine which unresponsive patients are conscious.

We may sometimes have "consciousness without access," that is, subjective experience of things that are too complex to fit into our working memory for later use.
For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn't imply that you had no conscious visual experience of it, merely that it wasn't stored in your working memory.
Should it count as forgetfulness rather than blindness?

When people ask about the meaning of life as if it were the job of our cosmos to give meaning to our existence, they're getting it backward.
It's not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe.

Contrast sapience (the ability to think intelligently) with sentience (the ability to subjectively experience qualia).
Humans have built our identity on being Homo sapiens.
I suggest that we rebrand ourselves as Homo sentiens.

Science gathers knowledge faster than society gathers wisdom.

Mindful optimism is the expectation that good things will happen if you plan carefully and work hard for them.

Develop positive visions for the future.
Positive visions form the foundation of all collaboration.
After all, why sacrifice something you have if you can't imagine the even greater gain that this will provide?
This means that we should be imagining positive futures not only for ourselves, but also for society and for humanity.

Do you want to own your technology or do you want your technology to own you?

The apparent consciousness of the octopus, with which we share no recent common ancestor, suggests consciousness is more than coincidence.
- THIS PRESUMES WE WEREN'T CREATED LOL

For ages, we tried to fly by copying what birds do, but when we finally did learn to fly, it was not by copying birds.
There are still things we don't understand about how birds fly, yet we can now fly further and faster than they can.

## ai researchers

Categorize machine learning researchers into five tribes: the symbolists, the evolutionaries, the Bayesians, the analogisers, and the connectionists.
Symbolists = Good Old-Fashioned AI.
Evolutionists start with a group of possible solutions to a problem and produce a second "generation" by introducing small random changes and removing the least effective solutions.
Bayesians make hypotheses about uncertain situations and updates its degree of belief in each hypothesis according to a mathematical rule when new evidence is provided.
Analogisers: new phenomenon is likely to follow the same behaviour as the previously observed phenomenon which is most like it.
Connectionists are trying to reverse engineer the brain by developing artificial neural networks.
Connectionism has been re-branded as deep learning, and it has been the most successful form of machine learning so far.

## Asserting Facts

[Peter Zeihan || Is the AI Revolution Here? - YouTube](https://www.youtube.com/watch?v=8jEmIDwqnL4)
- review for content on essay

[Meta disbanded its Responsible AI team | Hacker News](https://news.ycombinator.com/item?id=38328355)
[Meta disbanded its Responsible AI team - The Verge](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence)
- comments have good opinions

## careers in light of ai

Career Advice: choose professions that seem unlikely to get automated in the near future. Example:
Teacher, nurse, doctor, dentist, scientist, entrepreneur, programmer, engineer, lawyer, social worker, clergy member, artist, hairdresser or massage therapist.
Does it require interacting with people and using social intelligence?
Does it involve creativity and coming up with clever solutions?
Does it require working in an unpredictable environment?
Don't be the radiologist who analyzes the medical images and gets replaced by IBM's Watson, but the doctor who orders the radiology analysis, discusses the results with the patient, and decides on the treatment plan.

During the Industrial Revolution, we started figuring out how to replace our muscles with machines, and people shifted into better-paying jobs where they used their minds more.
Now we're gradually figuring out how to replace our minds by machines. If we ultimately succeed in this, then what jobs are left for us?

The vast majority of today's occupations are ones that already existed a century ago, and when we sort them by the number of jobs they provide, we have to go all the way down to twenty-first place in the list until we encounter a new occupation: software developers, who make up less than 1% of the U.S. job market.

The main trend on the job market isn't that we're moving into entirely new professions. Rather, we're crowding into those pieces of terrain that haven't yet been submerged by the rising tide of technology!

Imagine two horses seeing an automobile in the year 1900.
"I'm sure there'll be new new jobs for horses that we haven't yet imagined. That's what's always happened before, like with the invention of the wheel and the plow."
People decided to take care of horses.
Can we similarly take care of our fellow humans in need?

## what is ai

Two very different types of artificial intelligence:
artificial narrow intelligence (ANI) - also known as weak AI
artificial general intelligence (AGI) - also known as strong or full AI
An AGI is an AI system which can carry out any cognitive function that a human can.

We can learn useful lessons from one activity and apply them to another.
A machine that learns snakes and ladders has to forget that in order to learn how to play Ludo.
This is called "catastrophic forgetting".

"Artificial intelligence"?:
Cars are not called artificial horses.
Planes are not called artificial birds.

The three biggest questions about artificial general intelligence (AGI) are:
1. Can we build one?
2. If so, when?
3. Will it be safe?
The first of these questions is the closest to having an answer, and that answer is "probably, as long as we don't go extinct first".

Artificial superintelligence (ASI) is generally known simply as superintelligence. It does not need the prefix "artificial" since there is no natural predecessor.

Neurons signals in our brain travel around 100 metres per second.
Synapse crossing is chemicals jumping across the gap.
Synapse jumping part is much slower than the electrical part.
Signals within computers typically travel at 200 million metres per second.
So a brain emulation AGI could operate 2 million times faster than a human.

When we create superintelligence, will we create one, two, or many?
The first superintelligence may take steps to prevent the creation of a competitor.

Superintelligence with human levels of cognitive ability would be a technological marvel, and a harbinger of things to come.
Superintelligence would be the game-changer.
Our future will depend on its decisions and its actions.

Transhumanists believe we have every right to enhance our physical and mental selves using whatever technology we can create - perhaps even that we have a moral duty to do so.

Humanity versus a fully fledged superintelligence with internet access would be like the Amish versus the US Army.

We can use AI to augment us rather than having to compete with it. This is called Intelligence Augmentation, or IA, and is also known as intelligence amplification.
The best chess players today are neither humans nor computers, but a combination of both - labelled "centaurs" by Gary Kasparov.

Musk believes that humans cannot afford to let machines get much smarter than them, and the only way to avoid that is to - at least partly - merge with them.

Asimov's Three Laws of Robotics:
How far into the future would a robot have to project the consequences of an individual action?
How would it assign probabilities to the various different possible outcomes?
Since there should be no time limit, a robot would be rendered inactive by the amount of calculation required prior to any action.

Humans - as well as the superintelligence - will evolve over time and would not want to be stuck with the particular ideas of morality and pragmatism which happened to prevail in the 21st century.

Few of us even know the names of Vasili Arkhipov and Stanislav Petrov, two men who quite literally saved the world during the Cuban missile crisis.
